### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.ai.textanalytics.aio.TextAnalyticsClient.analyze_sentiment
  - azure.ai.textanalytics.aio.TextAnalyticsClient.detect_languages
  - azure.ai.textanalytics.aio.TextAnalyticsClient.extract_key_phrases
  - azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_entities
  - azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_linked_entities
  - azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_pii_entities
  class: azure.ai.textanalytics.aio.TextAnalyticsClient
  example:
  - "Creating the TextAnalyticsClient with endpoint and subscription key.<!--[!code-python[Main](les\\\
    async_samples\\sample_authentication_async.py )]-->\n\n<!-- literal_block {\"\
    ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [],\
    \ \"source\": \"D:\\\\a\\\\1\\\\s\\\\source_code\\\\6\\\\azure-ai-textanalytics-1.0.0b1\\\
    \\samples\\\\async_samples\\\\sample_authentication_async.py\", \"xml:space\"\
    : \"preserve\", \"language\": \"python\", \"linenos\": false, \"highlight_args\"\
    : {\"linenostart\": 1}} -->\n\n````python\n\n   from azure.ai.textanalytics.aio\
    \ import TextAnalyticsClient\n   endpoint = os.getenv(\"AZURE_TEXT_ANALYTICS_ENDPOINT\"\
    )\n   key = os.getenv(\"AZURE_TEXT_ANALYTICS_KEY\")\n\n   text_analytics_client\
    \ = TextAnalyticsClient(endpoint, key)\n\n   ````\n\nCreating the TextAnalyticsClient\
    \ with endpoint and token credential from Azure Active Directory.<!--[!code-python[Main](les\\\
    async_samples\\sample_authentication_async.py )]-->\n\n<!-- literal_block {\"\
    ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [],\
    \ \"source\": \"D:\\\\a\\\\1\\\\s\\\\source_code\\\\6\\\\azure-ai-textanalytics-1.0.0b1\\\
    \\samples\\\\async_samples\\\\sample_authentication_async.py\", \"xml:space\"\
    : \"preserve\", \"language\": \"python\", \"linenos\": false, \"highlight_args\"\
    : {\"linenostart\": 1}} -->\n\n````python\n\n   from azure.ai.textanalytics.aio\
    \ import TextAnalyticsClient\n   from azure.identity.aio import DefaultAzureCredential\n\
    \n   endpoint = os.getenv(\"AZURE_TEXT_ANALYTICS_ENDPOINT\")\n   credential =\
    \ DefaultAzureCredential()\n\n   text_analytics_client = TextAnalyticsClient(endpoint,\
    \ credential=credential)\n\n   ````\n"
  fullName: azure.ai.textanalytics.aio.TextAnalyticsClient
  inheritance:
  - inheritance:
    - type: builtins.object
    type: azure.ai.textanalytics.aio._base_client_async.AsyncTextAnalyticsClientBase
  langs:
  - python
  module: azure.ai.textanalytics.aio
  name: TextAnalyticsClient
  summary: 'The Text Analytics API is a suite of text analytics web services built
    with best-in-class

    Microsoft machine learning algorithms. The API can be used to analyze unstructured
    text for

    tasks such as sentiment analysis, key phrase extraction, and language detection.
    No training data

    is needed to use this API - just bring your text data. This API uses advanced
    natural language

    processing techniques to deliver best in class predictions.


    Further documentation can be found in

    [https://docs.microsoft.com/azure/cognitive-services/text-analytics/overview](https://docs.microsoft.com/azure/cognitive-services/text-analytics/overview)'
  syntax:
    content: TextAnalyticsClient(endpoint, credential, **kwargs)
    parameters:
    - description: 'Supported Cognitive Services or Text Analytics resource

        endpoints (protocol and hostname, for example: [https://westus2.api.cognitive.microsoft.com](https://westus2.api.cognitive.microsoft.com)).'
      id: endpoint
      type:
      - str
    - description: 'Credentials needed for the client to connect to Azure.

        This can be the cognitive services/text analytics subscription key or a token
        credential

        from azure.identity.'
      id: credential
      type:
      - str
      - azure.core.credentials.TokenCredential
  type: class
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient
- class: azure.ai.textanalytics.aio.TextAnalyticsClient
  example:
  - "Analyze sentiment in a batch of documents.<!--[!code-python[Main](les\\async_samples\\\
    sample_analyze_sentiment_async.py )]-->\n\n<!-- literal_block {\"ids\": [], \"\
    classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\":\
    \ \"D:\\\\a\\\\1\\\\s\\\\source_code\\\\6\\\\azure-ai-textanalytics-1.0.0b1\\\\\
    samples\\\\async_samples\\\\sample_analyze_sentiment_async.py\", \"xml:space\"\
    : \"preserve\", \"language\": \"python\", \"linenos\": false, \"highlight_args\"\
    : {\"linenostart\": 1}} -->\n\n````python\n\n   from azure.ai.textanalytics.aio\
    \ import TextAnalyticsClient\n   text_analytics_client = TextAnalyticsClient(endpoint=self.endpoint,\
    \ credential=self.key)\n   documents = [\n       \"I had the best day of my life.\"\
    ,\n       \"This was a waste of my time. The speaker put me to sleep.\",\n   \
    \    \"No tengo dinero ni nada que dar...\",\n       \"L'h\xF4tel n'\xE9tait pas\
    \ tr\xE8s confortable. L'\xE9clairage \xE9tait trop sombre.\"\n   ]\n\n   async\
    \ with text_analytics_client:\n       result = await text_analytics_client.analyze_sentiment(documents)\n\
    \n   docs = [doc for doc in result if not doc.is_error]\n\n   for idx, doc in\
    \ enumerate(docs):\n       print(\"Document text: {}\".format(documents[idx]))\n\
    \       print(\"Overall sentiment: {}\".format(doc.sentiment))\n\n   ````\n"
  exceptions:
  - description: ''
    type: ~azure.core.exceptions.HttpResponseError
  fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.analyze_sentiment
  langs:
  - python
  module: azure.ai.textanalytics.aio
  name: 'analyze_sentiment(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.AnalyzeSentimentResult,
    azure.ai.textanalytics._models.DocumentError]]'
  summary: 'Analyze sentiment for a batch of documents.


    Returns a sentiment prediction, as well as sentiment scores for

    each sentiment class (Positive, Negative, and Neutral) for the document

    and each sentence within it. See [https://aka.ms/talangs](https://aka.ms/talangs)
    for the list

    of enabled languages.'
  syntax:
    content: 'analyze_sentiment(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
      typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] =
      None, **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.AnalyzeSentimentResult,
      azure.ai.textanalytics._models.DocumentError]]'
    parameters:
    - description: 'The set of documents to process as part of this batch.

        If you wish to specify the ID and language on a per-item basis you must

        use as input a list[TextDocumentInput] or a list of dict representations of

        TextDocumentInput, like *{"id": "1", "language": "en", "text": "hello world"}*.'
      id: inputs
      type:
      - list[str]
      - list[azure.ai.textanalytics.TextDocumentInput]
    - description: 'The 2 letter ISO 639-1 representation of language for the

        entire batch. For example, use "en" for English; "es" for Spanish etc.

        If not set, uses "en" for English as default. Per-document language will

        take precedence over whole batch language.'
      id: language
      type:
      - str
    return:
      description: 'The combined list of AnalyzeSentimentResults and DocumentErrors
        in the order

        the original documents were passed in.'
      type:
      - list[azure.ai.textanalytics.AnalyzeSentimentResult,azure.ai.textanalytics.DocumentError]
  type: method
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.analyze_sentiment
- class: azure.ai.textanalytics.aio.TextAnalyticsClient
  example:
  - "Detecting language in a batch of documents.<!--[!code-python[Main](les\\async_samples\\\
    sample_detect_languages_async.py )]-->\n\n<!-- literal_block {\"ids\": [], \"\
    classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\":\
    \ \"D:\\\\a\\\\1\\\\s\\\\source_code\\\\6\\\\azure-ai-textanalytics-1.0.0b1\\\\\
    samples\\\\async_samples\\\\sample_detect_languages_async.py\", \"xml:space\"\
    : \"preserve\", \"language\": \"python\", \"linenos\": false, \"highlight_args\"\
    : {\"linenostart\": 1}} -->\n\n````python\n\n   from azure.ai.textanalytics.aio\
    \ import TextAnalyticsClient\n   text_analytics_client = TextAnalyticsClient(endpoint=self.endpoint,\
    \ credential=self.key)\n   documents = [\n       \"This document is written in\
    \ English.\",\n       \"Este es un document escrito en Espa\xF1ol.\",\n      \
    \ \"\u8FD9\u662F\u4E00\u4E2A\u7528\u4E2D\u6587\u5199\u7684\u6587\u4EF6\",\n  \
    \     \"Dies ist ein Dokument in englischer Sprache.\",\n       \"Detta \xE4r\
    \ ett dokument skrivet p\xE5 engelska.\"\n   ]\n   async with text_analytics_client:\n\
    \       result = await text_analytics_client.detect_languages(documents)\n\n \
    \  for idx, doc in enumerate(result):\n       if not doc.is_error:\n         \
    \  print(\"Document text: {}\".format(documents[idx]))\n           print(\"Language\
    \ detected: {}\".format(doc.primary_language.name))\n           print(\"ISO6391\
    \ name: {}\".format(doc.primary_language.iso6391_name))\n           print(\"Confidence\
    \ score: {}\\n\".format(doc.primary_language.score))\n       if doc.is_error:\n\
    \           print(doc.id, doc.error)\n\n   ````\n"
  exceptions:
  - description: ''
    type: ~azure.core.exceptions.HttpResponseError
  fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.detect_languages
  langs:
  - python
  module: azure.ai.textanalytics.aio
  name: 'detect_languages(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.DetectLanguageInput],
    typing.List[typing.Dict[str, str]]], country_hint: typing.Union[str, NoneType]
    = None, **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.DetectLanguageResult,
    azure.ai.textanalytics._models.DocumentError]]'
  summary: 'Detects Language for a batch of documents.


    Returns the detected language and a numeric score between zero and

    one. Scores close to one indicate 100% certainty that the identified

    language is true. See [https://aka.ms/talangs](https://aka.ms/talangs) for the
    list of enabled languages.'
  syntax:
    content: 'detect_languages(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.DetectLanguageInput],
      typing.List[typing.Dict[str, str]]], country_hint: typing.Union[str, NoneType]
      = None, **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.DetectLanguageResult,
      azure.ai.textanalytics._models.DocumentError]]'
    parameters:
    - description: 'The set of documents to process as part of this batch.

        If you wish to specify the ID and country_hint on a per-item basis you must

        use as input a list[DetectLanguageInput] or a list of dict representations
        of

        DetectLanguageInput, like *{"id": "1", "country_hint": "us", "text": "hello
        world"}*.'
      id: inputs
      type:
      - list[str]
      - list[azure.ai.textanalytics.DetectLanguageInput]
    - description: 'A country hint for the entire batch. Accepts two

        letter country codes specified by ISO 3166-1 alpha-2. Per-document

        country hints will take precedence over whole batch hints. Defaults to

        "US". If you don''t want to use a country hint, pass the empty string "".'
      id: country_hint
      type:
      - str
    return:
      description: 'The combined list of DetectLanguageResults and DocumentErrors
        in the order

        the original documents were passed in.'
      type:
      - list[azure.ai.textanalytics.DetectLanguageResult,azure.ai.textanalytics.DocumentError]
  type: method
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.detect_languages
- class: azure.ai.textanalytics.aio.TextAnalyticsClient
  example:
  - "Extract the key phrases in a batch of documents.<!--[!code-python[Main](les\\\
    async_samples\\sample_extract_key_phrases_async.py )]-->\n\n<!-- literal_block\
    \ {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\"\
    : [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\source_code\\\\6\\\\azure-ai-textanalytics-1.0.0b1\\\
    \\samples\\\\async_samples\\\\sample_extract_key_phrases_async.py\", \"xml:space\"\
    : \"preserve\", \"language\": \"python\", \"linenos\": false, \"highlight_args\"\
    : {\"linenostart\": 1}} -->\n\n````python\n\n   from azure.ai.textanalytics.aio\
    \ import TextAnalyticsClient\n   text_analytics_client = TextAnalyticsClient(endpoint=self.endpoint,\
    \ credential=self.key)\n   documents = [\n       \"Redmond is a city in King County,\
    \ Washington, United States, located 15 miles east of Seattle.\",\n       \"I\
    \ need to take my cat to the veterinarian.\",\n       \"I will travel to South\
    \ America in the summer.\",\n   ]\n\n   async with text_analytics_client:\n  \
    \     result = await text_analytics_client.extract_key_phrases(documents)\n\n\
    \   for doc in result:\n       if not doc.is_error:\n           print(doc.key_phrases)\n\
    \       if doc.is_error:\n           print(doc.id, doc.error)\n\n   ````\n"
  exceptions:
  - description: ''
    type: ~azure.core.exceptions.HttpResponseError
  fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.extract_key_phrases
  langs:
  - python
  module: azure.ai.textanalytics.aio
  name: 'extract_key_phrases(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.ExtractKeyPhrasesResult,
    azure.ai.textanalytics._models.DocumentError]]'
  summary: 'Extract Key Phrases from a batch of documents.


    Returns a list of strings denoting the key phrases in the input

    text. See [https://aka.ms/talangs](https://aka.ms/talangs) for the list of enabled

    languages.'
  syntax:
    content: 'extract_key_phrases(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
      typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] =
      None, **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.ExtractKeyPhrasesResult,
      azure.ai.textanalytics._models.DocumentError]]'
    parameters:
    - description: 'The set of documents to process as part of this batch.

        If you wish to specify the ID and language on a per-item basis you must

        use as input a list[TextDocumentInput] or a list of dict representations of

        TextDocumentInput, like *{"id": "1", "language": "en", "text": "hello world"}*.'
      id: inputs
      type:
      - list[str]
      - list[azure.ai.textanalytics.TextDocumentInput]
    - description: 'The 2 letter ISO 639-1 representation of language for the

        entire batch. For example, use "en" for English; "es" for Spanish etc.

        If not set, uses "en" for English as default. Per-document language will

        take precedence over whole batch language.'
      id: language
      type:
      - str
    return:
      description: 'The combined list of ExtractKeyPhrasesResults and DocumentErrors
        in the order

        the original documents were passed in.'
      type:
      - list[azure.ai.textanalytics.ExtractKeyPhrasesResult,azure.ai.textanalytics.DocumentError]
  type: method
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.extract_key_phrases
- class: azure.ai.textanalytics.aio.TextAnalyticsClient
  example:
  - "Recognize entities in a batch of documents.<!--[!code-python[Main](les\\async_samples\\\
    sample_recognize_entities_async.py )]-->\n\n<!-- literal_block {\"ids\": [], \"\
    classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\":\
    \ \"D:\\\\a\\\\1\\\\s\\\\source_code\\\\6\\\\azure-ai-textanalytics-1.0.0b1\\\\\
    samples\\\\async_samples\\\\sample_recognize_entities_async.py\", \"xml:space\"\
    : \"preserve\", \"language\": \"python\", \"linenos\": false, \"highlight_args\"\
    : {\"linenostart\": 1}} -->\n\n````python\n\n   from azure.ai.textanalytics.aio\
    \ import TextAnalyticsClient\n   text_analytics_client = TextAnalyticsClient(endpoint=self.endpoint,\
    \ credential=self.key)\n   documents = [\n       \"Microsoft was founded by Bill\
    \ Gates and Paul Allen.\",\n       \"I had a wonderful trip to Seattle last week.\"\
    ,\n       \"I visited the Space Needle 2 times.\",\n   ]\n\n   async with text_analytics_client:\n\
    \       result = await text_analytics_client.recognize_entities(documents)\n\n\
    \   docs = [doc for doc in result if not doc.is_error]\n\n   for idx, doc in enumerate(docs):\n\
    \       print(\"\\nDocument text: {}\".format(documents[idx]))\n       for entity\
    \ in doc.entities:\n           print(\"Entity: \\t\", entity.text, \"\\tType:\
    \ \\t\", entity.type,\n                 \"\\tConfidence Score: \\t\", round(entity.score,\
    \ 3))\n\n   ````\n"
  exceptions:
  - description: ''
    type: ~azure.core.exceptions.HttpResponseError
  fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_entities
  langs:
  - python
  module: azure.ai.textanalytics.aio
  name: 'recognize_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizeEntitiesResult,
    azure.ai.textanalytics._models.DocumentError]]'
  summary: 'Named Entity Recognition for a batch of documents.


    Returns a list of general named entities in a given document.

    For the list of supported entity types, check: [https://aka.ms/taner](https://aka.ms/taner)

    For the list of enabled languages, check: [https://aka.ms/talangs](https://aka.ms/talangs)'
  syntax:
    content: 'recognize_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
      typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] =
      None, **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizeEntitiesResult,
      azure.ai.textanalytics._models.DocumentError]]'
    parameters:
    - description: 'The set of documents to process as part of this batch.

        If you wish to specify the ID and language on a per-item basis you must

        use as input a list[TextDocumentInput] or a list of dict representations of

        TextDocumentInput, like *{"id": "1", "language": "en", "text": "hello world"}*.'
      id: inputs
      type:
      - list[str]
      - list[azure.ai.textanalytics.TextDocumentInput]
    - description: 'The 2 letter ISO 639-1 representation of language for the

        entire batch. For example, use "en" for English; "es" for Spanish etc.

        If not set, uses "en" for English as default. Per-document language will

        take precedence over whole batch language.'
      id: language
      type:
      - str
    return:
      description: 'The combined list of RecognizeEntitiesResults and DocumentErrors
        in the order

        the original documents were passed in.'
      type:
      - list[azure.ai.textanalytics.RecognizeEntitiesResult,azure.ai.textanalytics.DocumentError]
  type: method
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_entities
- class: azure.ai.textanalytics.aio.TextAnalyticsClient
  example:
  - "Recognize linked entities in a batch of documents.<!--[!code-python[Main](les\\\
    async_samples\\sample_recognize_linked_entities_async.py )]-->\n\n<!-- literal_block\
    \ {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\"\
    : [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\source_code\\\\6\\\\azure-ai-textanalytics-1.0.0b1\\\
    \\samples\\\\async_samples\\\\sample_recognize_linked_entities_async.py\", \"\
    xml:space\": \"preserve\", \"language\": \"python\", \"linenos\": false, \"highlight_args\"\
    : {\"linenostart\": 1}} -->\n\n````python\n\n   from azure.ai.textanalytics.aio\
    \ import TextAnalyticsClient\n   text_analytics_client = TextAnalyticsClient(endpoint=self.endpoint,\
    \ credential=self.key)\n   documents = [\n       \"Microsoft moved its headquarters\
    \ to Bellevue, Washington in January 1979.\",\n       \"Steve Ballmer stepped\
    \ down as CEO of Microsoft and was succeeded by Satya Nadella.\",\n       \"Microsoft\
    \ super\xF3 a Apple Inc. como la compa\xF1\xEDa m\xE1s valiosa que cotiza en bolsa\
    \ en el mundo.\",\n   ]\n\n   async with text_analytics_client:\n       result\
    \ = await text_analytics_client.recognize_linked_entities(documents)\n\n   docs\
    \ = [doc for doc in result if not doc.is_error]\n\n   for idx, doc in enumerate(docs):\n\
    \       print(\"Document text: {}\\n\".format(documents[idx]))\n       for entity\
    \ in doc.entities:\n           print(\"Entity: {}\".format(entity.name))\n   \
    \        print(\"Url: {}\".format(entity.url))\n           print(\"Data Source:\
    \ {}\".format(entity.data_source))\n           for match in entity.matches:\n\
    \               print(\"Score: {0:.3f}\".format(match.score))\n              \
    \ print(\"Offset: {}\".format(match.offset))\n               print(\"Length: {}\\\
    n\".format(match.length))\n       print(\"------------------------------------------\"\
    )\n\n   ````\n"
  exceptions:
  - description: ''
    type: ~azure.core.exceptions.HttpResponseError
  fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_linked_entities
  langs:
  - python
  module: azure.ai.textanalytics.aio
  name: 'recognize_linked_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizeLinkedEntitiesResult,
    azure.ai.textanalytics._models.DocumentError]]'
  summary: 'Recognize linked entities from a well-known knowledge base for a batch
    of documents.


    Returns a list of recognized entities with links to a

    well-known knowledge base. See [https://aka.ms/talangs](https://aka.ms/talangs)
    for

    supported languages in Text Analytics API.'
  syntax:
    content: 'recognize_linked_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
      typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] =
      None, **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizeLinkedEntitiesResult,
      azure.ai.textanalytics._models.DocumentError]]'
    parameters:
    - description: 'The set of documents to process as part of this batch.

        If you wish to specify the ID and language on a per-item basis you must

        use as input a list[TextDocumentInput] or a list of dict representations of

        TextDocumentInput, like *{"id": "1", "language": "en", "text": "hello world"}*.'
      id: inputs
      type:
      - list[str]
      - list[azure.ai.textanalytics.TextDocumentInput]
    - description: 'The 2 letter ISO 639-1 representation of language for the

        entire batch. For example, use "en" for English; "es" for Spanish etc.

        If not set, uses "en" for English as default. Per-document language will

        take precedence over whole batch language.'
      id: language
      type:
      - str
    return:
      description: 'The combined list of RecognizeLinkedEntitiesResults and DocumentErrors
        in the order

        the original documents were passed in.'
      type:
      - list[azure.ai.textanalytics.RecognizeLinkedEntitiesResult,azure.ai.textanalytics.DocumentError]
  type: method
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_linked_entities
- class: azure.ai.textanalytics.aio.TextAnalyticsClient
  example:
  - "Recognize personally identifiable information entities in a batch of documents.<!--[!code-python[Main](les\\\
    async_samples\\sample_recognize_pii_entities_async.py )]-->\n\n<!-- literal_block\
    \ {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\"\
    : [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\source_code\\\\6\\\\azure-ai-textanalytics-1.0.0b1\\\
    \\samples\\\\async_samples\\\\sample_recognize_pii_entities_async.py\", \"xml:space\"\
    : \"preserve\", \"language\": \"python\", \"linenos\": false, \"highlight_args\"\
    : {\"linenostart\": 1}} -->\n\n````python\n\n   from azure.ai.textanalytics.aio\
    \ import TextAnalyticsClient\n   text_analytics_client = TextAnalyticsClient(endpoint=self.endpoint,\
    \ credential=self.key)\n   documents = [\n       \"The employee's SSN is 555-55-5555.\"\
    ,\n       \"Your ABA number - 111000025 - is the first 9 digits in the lower left\
    \ hand corner of your personal check.\",\n       \"Is 998.214.865-68 your Brazilian\
    \ CPF number?\"\n   ]\n\n   async with text_analytics_client:\n       result =\
    \ await text_analytics_client.recognize_pii_entities(documents)\n\n   docs = [doc\
    \ for doc in result if not doc.is_error]\n\n   for idx, doc in enumerate(docs):\n\
    \       print(\"Document text: {}\".format(documents[idx]))\n       for entity\
    \ in doc.entities:\n           print(\"Entity: {}\".format(entity.text))\n   \
    \        print(\"Type: {}\".format(entity.type))\n           print(\"Confidence\
    \ Score: {}\\n\".format(entity.score))\n\n   ````\n"
  exceptions:
  - description: ''
    type: ~azure.core.exceptions.HttpResponseError
  fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_pii_entities
  langs:
  - python
  module: azure.ai.textanalytics.aio
  name: 'recognize_pii_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizePiiEntitiesResult,
    azure.ai.textanalytics._models.DocumentError]]'
  summary: 'Recognize entities containing personal information for a batch of documents.


    Returns a list of personal information entities ("SSN",

    "Bank Account", etc) in the document.  For the list of supported entity types,

    check [https://aka.ms/tanerpii](https://aka.ms/tanerpii). See [https://aka.ms/talangs](https://aka.ms/talangs)

    for the list of enabled languages.'
  syntax:
    content: 'recognize_pii_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
      typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] =
      None, **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizePiiEntitiesResult,
      azure.ai.textanalytics._models.DocumentError]]'
    parameters:
    - description: 'The set of documents to process as part of this batch.

        If you wish to specify the ID and language on a per-item basis you must

        use as input a list[TextDocumentInput] or a list of dict representations of

        TextDocumentInput, like *{"id": "1", "language": "en", "text": "hello world"}*.'
      id: inputs
      type:
      - list[str]
      - list[azure.ai.textanalytics.TextDocumentInput]
    - description: 'The 2 letter ISO 639-1 representation of language for the

        entire batch. For example, use "en" for English; "es" for Spanish etc.

        If not set, uses "en" for English as default. Per-document language will

        take precedence over whole batch language.'
      id: language
      type:
      - str
    return:
      description: 'The combined list of RecognizePiiEntitiesResults and DocumentErrors
        in the order

        the original documents were passed in.'
      type:
      - list[azure.ai.textanalytics.RecognizePiiEntitiesResult,azure.ai.textanalytics.DocumentError]
  type: method
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_pii_entities
references:
- fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.analyze_sentiment
  isExternal: false
  name: 'analyze_sentiment(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.AnalyzeSentimentResult,
    azure.ai.textanalytics._models.DocumentError]]'
  parent: azure.ai.textanalytics.aio.TextAnalyticsClient
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.analyze_sentiment
- fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.detect_languages
  isExternal: false
  name: 'detect_languages(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.DetectLanguageInput],
    typing.List[typing.Dict[str, str]]], country_hint: typing.Union[str, NoneType]
    = None, **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.DetectLanguageResult,
    azure.ai.textanalytics._models.DocumentError]]'
  parent: azure.ai.textanalytics.aio.TextAnalyticsClient
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.detect_languages
- fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.extract_key_phrases
  isExternal: false
  name: 'extract_key_phrases(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.ExtractKeyPhrasesResult,
    azure.ai.textanalytics._models.DocumentError]]'
  parent: azure.ai.textanalytics.aio.TextAnalyticsClient
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.extract_key_phrases
- fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_entities
  isExternal: false
  name: 'recognize_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizeEntitiesResult,
    azure.ai.textanalytics._models.DocumentError]]'
  parent: azure.ai.textanalytics.aio.TextAnalyticsClient
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_entities
- fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_linked_entities
  isExternal: false
  name: 'recognize_linked_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizeLinkedEntitiesResult,
    azure.ai.textanalytics._models.DocumentError]]'
  parent: azure.ai.textanalytics.aio.TextAnalyticsClient
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_linked_entities
- fullName: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_pii_entities
  isExternal: false
  name: 'recognize_pii_entities(inputs: typing.Union[typing.List[str], typing.List[azure.ai.textanalytics._models.TextDocumentInput],
    typing.List[typing.Dict[str, str]]], language: typing.Union[str, NoneType] = None,
    **kwargs: typing.Any) -> typing.List[typing.Union[azure.ai.textanalytics._models.RecognizePiiEntitiesResult,
    azure.ai.textanalytics._models.DocumentError]]'
  parent: azure.ai.textanalytics.aio.TextAnalyticsClient
  uid: azure.ai.textanalytics.aio.TextAnalyticsClient.recognize_pii_entities
- fullName: list[str]
  name: list[str]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  - fullName: ']'
    name: ']'
  uid: list[str]
- fullName: list[azure.ai.textanalytics.TextDocumentInput]
  name: list[TextDocumentInput]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: azure.ai.textanalytics.TextDocumentInput
    name: TextDocumentInput
    uid: azure.ai.textanalytics.TextDocumentInput
  - fullName: ']'
    name: ']'
  uid: list[azure.ai.textanalytics.TextDocumentInput]
- fullName: list[azure.ai.textanalytics.AnalyzeSentimentResult,azure.ai.textanalytics.DocumentError]
  name: list[DocumentError]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: azure.ai.textanalytics.AnalyzeSentimentResult,azure.ai.textanalytics.DocumentError
    name: DocumentError
    uid: azure.ai.textanalytics.AnalyzeSentimentResult,azure.ai.textanalytics.DocumentError
  - fullName: ']'
    name: ']'
  uid: list[azure.ai.textanalytics.AnalyzeSentimentResult,azure.ai.textanalytics.DocumentError]
- fullName: list[azure.ai.textanalytics.DetectLanguageInput]
  name: list[DetectLanguageInput]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: azure.ai.textanalytics.DetectLanguageInput
    name: DetectLanguageInput
    uid: azure.ai.textanalytics.DetectLanguageInput
  - fullName: ']'
    name: ']'
  uid: list[azure.ai.textanalytics.DetectLanguageInput]
- fullName: list[azure.ai.textanalytics.DetectLanguageResult,azure.ai.textanalytics.DocumentError]
  name: list[DocumentError]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: azure.ai.textanalytics.DetectLanguageResult,azure.ai.textanalytics.DocumentError
    name: DocumentError
    uid: azure.ai.textanalytics.DetectLanguageResult,azure.ai.textanalytics.DocumentError
  - fullName: ']'
    name: ']'
  uid: list[azure.ai.textanalytics.DetectLanguageResult,azure.ai.textanalytics.DocumentError]
- fullName: list[azure.ai.textanalytics.ExtractKeyPhrasesResult,azure.ai.textanalytics.DocumentError]
  name: list[DocumentError]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: azure.ai.textanalytics.ExtractKeyPhrasesResult,azure.ai.textanalytics.DocumentError
    name: DocumentError
    uid: azure.ai.textanalytics.ExtractKeyPhrasesResult,azure.ai.textanalytics.DocumentError
  - fullName: ']'
    name: ']'
  uid: list[azure.ai.textanalytics.ExtractKeyPhrasesResult,azure.ai.textanalytics.DocumentError]
- fullName: list[azure.ai.textanalytics.RecognizeEntitiesResult,azure.ai.textanalytics.DocumentError]
  name: list[DocumentError]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: azure.ai.textanalytics.RecognizeEntitiesResult,azure.ai.textanalytics.DocumentError
    name: DocumentError
    uid: azure.ai.textanalytics.RecognizeEntitiesResult,azure.ai.textanalytics.DocumentError
  - fullName: ']'
    name: ']'
  uid: list[azure.ai.textanalytics.RecognizeEntitiesResult,azure.ai.textanalytics.DocumentError]
- fullName: list[azure.ai.textanalytics.RecognizeLinkedEntitiesResult,azure.ai.textanalytics.DocumentError]
  name: list[DocumentError]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: azure.ai.textanalytics.RecognizeLinkedEntitiesResult,azure.ai.textanalytics.DocumentError
    name: DocumentError
    uid: azure.ai.textanalytics.RecognizeLinkedEntitiesResult,azure.ai.textanalytics.DocumentError
  - fullName: ']'
    name: ']'
  uid: list[azure.ai.textanalytics.RecognizeLinkedEntitiesResult,azure.ai.textanalytics.DocumentError]
- fullName: list[azure.ai.textanalytics.RecognizePiiEntitiesResult,azure.ai.textanalytics.DocumentError]
  name: list[DocumentError]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: azure.ai.textanalytics.RecognizePiiEntitiesResult,azure.ai.textanalytics.DocumentError
    name: DocumentError
    uid: azure.ai.textanalytics.RecognizePiiEntitiesResult,azure.ai.textanalytics.DocumentError
  - fullName: ']'
    name: ']'
  uid: list[azure.ai.textanalytics.RecognizePiiEntitiesResult,azure.ai.textanalytics.DocumentError]
